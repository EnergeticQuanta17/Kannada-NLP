1. UNITS_IN_LSTM_LAYER
    See the number of parameters used for each UNITS_IN_LSTM_LAYER

2. Attention mechanism after to extract features from concatenated hidden states

3. Custom training accuracy and loss

4. See for which sentences it is not doing good (Broader)
    After doing that see individual words doing bad

    i.e. first count the number of errors / no of words in sentence
        Do inference from high ratio to low ratio

5. Inference

6. Confusion matrix

7. How to connect to tensorboard

8. Check how much vanishing gradients happen for long seequences of sentences compared to shorter seqeuence of sentences.
    Check how low the gradients are for long sequence inputs

    (Slide 48) of 05_rnns.pdf says long term components goes exponentially fast to norm 0    

    Check in which cases it explodes VS which cases it underflows and becomes 0

9. How much change in each of the weights?

10. Drop all sentences with length more than 35 ig 
    See percentage and decide

11. Some bigger task model must have a hidden sandhi splitter inside it
    Or else how is that high accuracy model achieving high accuracy

12. Agreement between the different tags in output sequence

13. Why is validation loss increasing rapidly when the learning rate is very low

14. See the confidence level of each of the output

15. 


*** Check how to print correct validation and test accuracy
    If we can fix this, the model itself can improve on its own ig

*** Something with the model itself I feel
    It can accept None as the first shape, then it must be BATCH_SIZE as the second dimension of the first layer shape right

*** 