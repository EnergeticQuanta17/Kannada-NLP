1. UNITS_IN_LSTM_LAYER
    See the number of parameters used for each UNITS_IN_LSTM_LAYER

2. Attention mechanism after to extract features from concatenated hidden states

3. Custom training accuracy and loss

4. See for which sentences it is not doing good (Broader)
    After doing that see individual words doing bad

    i.e. first count the number of errors / no of words in sentence
        Do inference from high ratio to low ratio

5. Inference

6. Confusion matrix

7. How to connect to tensorboard
    Use WeightsAndBias Website

8. Check how much vanishing gradients happen for long seequences of sentences compared to shorter seqeuence of sentences.
    Check how low the gradients are for long sequence inputs

    (Slide 48) of 05_rnns.pdf says long term components goes exponentially fast to norm 0    

    Check in which cases it explodes VS which cases it underflows and becomes 0

9. How much change in each of the weights?

10. Drop all sentences with length more than 35 ig 
    See percentage and decide
    See the 90% cutoff

11. Some bigger task model must have a hidden sandhi splitter inside it
    Or else how is that high accuracy model achieving high accuracy

12. Agreement between the different tags in output sequence

13. Why is validation loss increasing rapidly when the learning rate is very low

14. See the confidence level of each of the output

15. How well does it do for broader categories of POS?

16. For multiple runs see where it tags wrongly for different sets of test sets

17. Compare this dataset with IIIT-H dataset

18. A way to use the other infomration in the annotated dataset

19. Teacher Forcing

20. Check how the words are being interpreted by the network in the initial stage

21. Calculate F1 score for each label separately

22. Find common error patterns

23. Do n-Fold split and collect data for particular folds

24. Find an encoder for Kananda specifically
    Eg. seq2seq

25. trainable=False

26. 

27. 

28. 

29. 

30. 




*** Check how to print correct validation and test accuracy
    If we can fix this, the model itself can improve on its own ig

*** Something with the model itself I feel
    It can accept None as the first shape, then it must be BATCH_SIZE as the second dimension of the first layer shape right

*** 