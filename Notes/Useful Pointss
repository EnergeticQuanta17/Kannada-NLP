The term "Dravidian" refers to a group of languages that share a common linguistic and cultural heritage. 

It also matters what probability prefers that POS tag
    And also compare with the 2nd best

Are all the sentences IID?
    If they are IID, then it is okay to shuffle

    Else we have to make validation and test set, separately contiguous

BLEU (Bilingual Evaluation Understudy) is a metric used to evaluate the quality of machine-generated translations by comparing them to one or more reference translations. It measures the similarity between the generated translation and the reference translations based on n-gram precision and brevity penalty.

Having high n-gram precision means that the generated translation contains a significant number of n-grams (contiguous sequences of n words) that exactly match those in the reference translation.

The hidden state acts as a memory that encodes information about the sequence seen so far.

Information Propagation: As the model processes each symbol, the hidden state is updated and carries information from previous time steps to the current time step. This allows the model to capture dependencies and contextual information across different positions or time steps in the sequence.

Bi-LSTM

Anaphora resolution
    which entity is being referred to


In the Long Short Term Memory (LSTM) architecture, there are three important components called gates: the input gate, output gate, and forget gate. These gates play a crucial role in controlling the flow of information into and out of the memory cells in an LSTM network.

Input Gate:

The input gate determines how much new information should be added to the memory cells.
It takes the current input and the previous hidden state as inputs and applies a sigmoid function to them.
The sigmoid function outputs values between 0 and 1, indicating how much of the new information should be kept (1) or discarded (0).
The input gate multiplies the current input with the output of the sigmoid function, which allows it to control the flow of new information into the memory cells.
Forget Gate:

The forget gate decides what information should be removed from the memory cells.
It takes the current input and the previous hidden state as inputs and applies a sigmoid function to them.
The sigmoid function produces values between 0 and 1, indicating how much of the previous memory contents should be retained (1) or forgotten (0).
The forget gate multiplies the previous cell state (memory) with the output of the sigmoid function, effectively allowing it to "forget" certain information from the previous memory.
Output Gate:

The output gate controls how much information from the memory cells should be exposed as the output of the LSTM at the current time step.
It takes the current input and the previous hidden state as inputs and applies a sigmoid function to them.
The sigmoid function produces values between 0 and 1, determining the amount of information to be exposed as the output.
The output gate multiplies the cell state (after passing through the activation function, which could be a hyperbolic tangent or ReLU) with the output of the sigmoid function.
The result is the output of the LSTM at the current time step.
By using these gates, the LSTM network can selectively control the flow of information through time. The input gate determines how much new information should be stored in the memory cells, the forget gate decides what information should be discarded, and the output gate determines how much information from the memory cells should be exposed as the output.

The interactions between the gates and the memory cells in an LSTM are multiplicative in nature. This means that the input, output, and forget gates provide continuous analogues of write, read, and reset operations for the memory cells. They control the interactions between the input, previous memory state, and current output, allowing the LSTM to effectively store, retrieve, and update information over time.

================
rnn_tutorial.pdf
================
    Ways to deal with sequence labelling
        Autoregressive models
        Feed-forward neural nets

        Linear Dynamic Systems
        Hidden Markov Models

        Recurrent Neural Networks
            Update the hidden state in a deterministic nonlinear Ways

            RNNs powerful because
                Distributed hidden state - allows to store lots of information about the past efficiently
                Non-linear dynamics to update hidden states
                No need to infer hidden state, purely deterministic 
                Weight sharing(I feel like weights must not be same and it must depend on context of the current sentence under observation)
            
            
    Has algorithm for Bidirectional (clean format)

    Vanishing Gradients mathematical represetntaion of the problem 
        Toy example
        Math

        Solutions
            Initialization + ReLUs
            Trick for exploding gradient: clipping
        
    
    Range of context is limited
        Because the influence of a given input hidden layer, the network output either decays or blows up
        Most effective solution --> LSTM

        can be thought as differential version of memory chips (this line was picked up from 2005s)

        Vanishing gradients problem can be illustrated

        Read slide27 math equations ON REVISIT

        GRU merges cell state and hidden state
            Much less parameters
            Combines the forget and input gates into a single update gate
        
    Implementation of RNN in TF is also done

================================
lec10new.pdf (Geoffery Hinton)
================================
    IS TOO GOOD

    It is because the RNN can represent any automaton, it is able to do anything that a computer can do
        Which is also connect wtith markov chains and of course RL

    Implement a RNN for binary addition - üòÅ

    Recurrent neural network can emulate a finite state automaton, but it is exponentially more powerful
        (Key to P vs. NP)
        But the thing is RNN has to square the number of states in trade for exponential increase in activity
    
    Read backprop through memory cell again

    Curvature matrices

    The key idea of echo state Networks
        Is this related to POS tagging?
    
Need to check for stemming and stuff (but getting correct word embeddings from the stem requires the word's POS tag)
    THINK HARD ABOUT THIS

========
05_rnns
========
    Examples of sequential data
        Speech Recognition
        Machine Translation
        Language Modeling
        Named Entity Recognition
        Sentiment Classification
        Video Activity Analysis

        Just write top 5 main uses of RNNs, find out from other papers also

    Inp, output can be different lengths in differnetn Examples
        Put this more cleanly
    
    Language concepts
        Words and Vocabulary
        Grammar and Sytax
        Semantics
        Discourse and Pragmatics
        Named Entities
        Sentiment and Opinion
    
    RNN
        Introduce Bias based on previous output
        RNN is a memory network
        Dynamic Hidden state
            Distributed Hidden state
            Non-linear dynamics
            Temporal and accumulative

        Memory-less include Autoregressive and FF neural network

        Short term dependencies

        Long term dependencies

        Feed forward do not take time dependencies into account

        (Slide 20) The same fucnction g and same set of parameters W are used at every time step (check if true, and variants)

        Selective summarization of input sequence in a fixed size state/hidden vector via a recurisve update

        Read the BPTT in RNN properly once after seeing video on it
            Nice equations and from-scratch-code is given for BPTT in RNN

    Some complex math explaining vanishing and exploding gradients
        MUST READ BEFORE WRITING IMPLEMENTATION PART

    Dealing with Exploding Gradients
        Gradient Clipping
            Introduces an additional hyperparameter, the "threshold"
        
    Dealing with Vanishing Gradient
        LSTM: Gating Mechanism

        (Slide 80) amazing explaination of Gating Mechanism
            But how is it actually done
    
            ----------------------------CONTINUE READING FROM SLIDE 81----------------------

========================
498_FA2019_lecture12
========================
    (Slide 16) 18 REALLY NICE EQUATIONS

    (Slide 49) min-char-rnn.py
        See the RNN code
    
    (Slide 63) Searching for Interpretatble Hidden Units
        Find out what are all those slides - Oh its in the paper

    READ THE PAPER "VISUALIZING AND UNDERSTANDING RECURRENT NETWORKS"
        1000-3000
    
    (Slide 84) to end
        VERY GOOD REPRESENTATION OF EQUTIONS FOR LSTM

    READ THE LAST PART CAREFULLY


