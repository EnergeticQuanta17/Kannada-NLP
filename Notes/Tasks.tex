Data Analysis 
	

fastText models on the dataset
	Can directly pass data ig

	https://fasttext.cc/docs/en/aligned-vectors.html



AutoNLP
	explore

Extract many many words from wikipedia of kannada
	Identify whether it is kannada using unicode and then store the sentence in utf-8

Train the embeddings for the remaining %age of words

Check that in different contexts the words have differnt meaning
	how does embedding take care of this?

See the internal implementation of BiDirectional layer

Do prefix matching for the words
	And print for each word, at each prefix length how many words match
	(Almost like stemming)

	Use the oov variable info
	
Find out a way in which we can find embeddings for unknown words from using partially known embeddings(49% one)

Teacher Forcing

Self-supervised tasks
	Learning from unlabelled data

	1. MULTI-MASK LANGUAGE MODELING
	2. NEXT SENTENCE PREDICTION (from the list of given options of sentences)
	3. 

BERT
	Pre-training
		1. Masked Language Modeling
			15% of the tokens are chosen
				mask 80% of the time
				replace them with random token 10% of the time
				keep them as is for 10% of the time
			Dense layer and softmax

		2. Next sentence prediction
	
	Position embeddings
		Position in the sentence
		Segment Embeddings --> sentence A or sentence B --> see how its actually implemented
		Token Embeddings --> [CLS] at front and [SEP] at end

		Take the sum? 
	
	Cross entropy loss and Binary loss for obj1 and obj2

	Fine Tuning BERT

Check why this error comes
	Some weights of the model checkpoint at Koodsml/KooBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']
	- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
	- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
	Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Koodsml/KooBERT and are newly initialized: ['classifier.weight', 'classifier.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']
	You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Niggetya Poddar
    Refer this paper for headings
